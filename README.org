# title shouldn't appear in toc
* Kaggle Predict Sales :noexport:

[[https://www.kaggle.com/c/competitive-data-science-predict-future-sales/overview][Competition link]]

* Contents :TOC:
- [[#data--eda][Data + EDA]]
- [[#results][Results]]
  - [[#summary][Summary]]
  - [[#baseline-model][Baseline model]]
  - [[#add-item-categories-and-various-time-features][Add item categories and various time features]]
  - [[#use-only-the-most-recent-years-data][Use only the most recent year's data]]
- [[#possible-improvements][Possible improvements]]
  - [[#improve-these-models][Improve these models]]
  - [[#multi-step-prediction][Multi-step prediction]]
  - [[#other-models][Other models]]
- [[#files-in-this-repository][Files in this Repository]]

* Data + EDA

Intro

* Results
** Summary

- Trained 3 models with xgboost: baseline, adding item category and various time features, and feature engineering + using only the most recent year's data (perhaps using only recent data improves performance).
- Last month of data used as test set, 2nd to last month used as validation set.
- Models were trained with default parameters, except for training with as many trees as need until overfitting (as measured by RSME on validation set).
- Models were evaluated using RSME. MAE and MAPE were also tracked, but in general these are poor metrics for this task: almost all item count values are 1 and so if the model makes even a slight error such as predicting 2 instead of 1, this is 100% error according to MAPE.
- The model using all of the data + feature engineering performed best. The baseline model immediately overfit, showing poor generalization to validation. The model using feature engineering + only the most recent year's data performed slightly worse on the test set than the model using all of the data.
- Even the best model had RMSE of roughly 10 on the test set. This isn't terrible, but is noticeably worse than the validation set (roughly 1.5). It could be that the majority of this error is coming from a single day in late October 2015 (see plots in subsequent sections). Googling Russian holidays didn't turn up anything obvious to explain this.
- Difficult to get an overall visualization of the prediction for count of sales for so many item-shop combinations. As a rough test, we plot predicted total sales and the true values for total sales on each day of the predicted month. We see decent qualitative agreement.

** Baseline model

Observations:

- Validation RMSE always much higher than train, overfitting/model not very predictive.
- Decent looking summarising graphs for validation and test.

Summary Statistics:

number of trees: 20

train-rmse: 1.80402	
validation-rmse: 6.08256

train-mape: 0.20434	
validation-mape: 0.17836

train-mae: 0.34221	
validation-mae: 0.50230

test rsme: 9.379496907843956

[[baseline_feature_importance.png]]

[[baseline_validation.png]]

[[baseline_test.png]]

** Add item categories and various time features

Observations:

- Excellent improvement in rmse on validation over baseline model.
- Not great rmse on test (not terrible).
- Otherwise, decent looking summarising graphs for validation and test.

Summary Statistics

number of trees: 98

train-rmse: 1.56050	
validation-rmse: 1.56527

train-mape: 0.21753	
validation-mape: 0.22912

train-mae: 0.36448	
validation-mae: 0.39369

test rsme: 9.754962031326457

[[add_features_feature_importance.png]]

[[add_features_validation.png]]

[[add_features_test.png]]

** Use only the most recent year's data

Observations:

- No major difference from using all of the data.
- If anything, RMSE on test worsens using only data from 2015.

Summary statistics:

number of trees: 39

train-rmse: 1.47490		
validation-rmse: 1.47621

train-mape: 0.17846	
validation-mape: 0.17519

train-mae: 0.29268	
validation-mae: 0.34397

test rmse: 10.024865745802856

[[add_features_2015_feature_importance.png]]

[[add_features_2015_validation.png]]

[[add_features_2015_test.png]]

* Possible improvements

Ran out of time, but here are some ways to improve:

** Improve these models
- Hyperparameter searches
- Look at days/items/shops where model performs worst for clues
- Add lag features
- Could see if making the data stationary helps
 
** Multi-step prediction
- We've done direct one-month ahead and two-month ahead prediction. Model can be adapted to direct N-month ahead predictions.
- Could try recursive N-month or N-day ahead predictions with this model.

** Other models
- ARIMA (maybe with just a few month's data)
- Prophet
- If time + compute, TFT looks cool and could be SOTA: https://arxiv.org/pdf/1912.09363v2.pdf
 
* Files in this Repository

- eda.ipynb
- clean_data.py

Run clean_data.py to create train_clean.csv, used in xgboost.ipynb  
  
- xgboost.ipynb
