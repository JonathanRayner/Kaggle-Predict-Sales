# title shouldn't appear in toc
* Kaggle 1C Sales Prediction :noexport:

- This notebook implements an XGBoost model to forecast various item sales at various stores in Russia, based on 3 years of sales data. 
- Challenge: EDA and build the best possible XGBoost forecasting model in 2 days.
- Current status: in rough testing, the model achieves ~1.5 RMSE in 1 month look-ahead forecasting, ~2.3 RMSE in 2 month look-ahead forecasting. This is relative to 1 sale per item per day being extremely common in the data, so this shows decent performance for a first pass.
- The data is taken from this [[https://www.kaggle.com/c/competitive-data-science-predict-future-sales/overview][kaggle competition]] (which has a slightly different objective).

* Contents :TOC:
- [[#data--eda][Data + EDA]]
  - [[#data][Data]]
  - [[#eda][EDA]]
- [[#results][Results]]
  - [[#summary][Summary]]
  - [[#baseline-model][Baseline model]]
  - [[#add-item-categories-and-various-time-features][Add item categories and various time features]]
  - [[#use-only-the-most-recent-years-data][Use only the most recent year's data]]
- [[#possible-improvements][Possible improvements]]
  - [[#improve-these-models][Improve these models]]
  - [[#multi-step-prediction][Multi-step prediction]]
  - [[#other-tools][Other Tools]]
- [[#files-in-this-repository][Files in this Repository]]

* Data + EDA
** Data

Data available [[https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data][here]]. Data gives count of sales of various items at various shops in Russia, across a 3 year period. Our goal is to predict item sales at these stores in future months.

** EDA

See eda.ipynb . Summarizing:

- There are a small number of duplicate rows. We remove these in cleaning.
- No missing values.
- dtypes are all float and int, once we convert dates to datetime object and use as index.
- Checking feature histograms:

[[file:figs/feature_hist.png]]

- Some shops have significantly more entries than others, some item categories appear significantly more often than others
- Low prices and low item sale counts appear much more often than higher values.
- We see spikes in number of entries each year in December.
- There is a single entry with negative item price - we remove this in cleaning.
- There are many negative item sales counts. Perhaps these are returns - we don't remove these.
- There is one item that is an extreme outlier in price and was only bought once ever. It appears to be some sort of corporate software. We remove this outlier.
- There are some outliers in total item sales in one day. In particular, there is an October 28, 2015 entry that is an extreme outlier: the price is completely different from other listings of this item at this shop, and there 2000 sales, which is an extreme outlier in all item sales. We remove this outlier, but don't know enough to remove any others. 
- There are shops and items that have noticeably strange behavior, such as much fewer sales in the most recent year. We identify these as shops and items not asked for in the test.csv file. We use these entries in our baseline model, but remove them in our subsequent models when we begin feature engineering.

* Results
** Summary

- Trained 3 models with xgboost: baseline, adding item category and various time features, and feature engineering + using only the most recent year's data (perhaps using only recent data improves performance).
- Last month of data used as test set, 2nd to last month used as validation set.
- Models were trained with default parameters, except for training with as many trees as need until overfitting (as measured by RSME on validation set).
- Models were evaluated using RSME. MAE and MAPE were also tracked, but may not be good metrics for this task: almost all item count values are 1 and so if the model makes even a slight error such as predicting 2 instead of 1, this is 100% error according to MAPE.
- The model using all of the data + feature engineering performed best. The baseline model immediately overfit, showing poor generalization to validation. The model using feature engineering + only the most recent year's data performed slightly worse on the test set than the model using all of the data (possibly due to higher weight on a single outlier). However, MAPE + MAE were better for the model using only the most recent year's data, which gives preliminary evidence that this may be the best model. 
- Even the best model had RMSE of roughly 2.3 on the test set. This isn't terrible, but is worse than the validation set (roughly 1.5). There could be a single outlier contributing most of this difference (see predicted vs. true scatter plots in subsections below). We need to look further into whether we can remove this outlier.
- We also plot predicted total sales and the true values for total sales on each day of the predicted month. We see decent qualitative agreement.

** Baseline model

Observations:

- Validation RMSE always much higher than train, overfitting/model not very predictive.
- Scatter plot of predicted vs. true shows distribution along the axes. This is because the model is over-predicting sales=1 (which is a very common value in the data) and also predicting values that are too high for many data points that have item sales=1 as their true value.
- Surprisingly decent looking total daily sales graphs for validation and test.

Summary Statistics:

number of trees: 20

train-rmse: 1.80402	
validation-rmse: 6.08256

train-mape: 0.20434	
validation-mape: 0.17836

train-mae: 0.34221	
validation-mae: 0.50230

test rsme: 2.419216650990203

[[file:figs/baseline_feature_importance.png]]

Validation set predicted values vs. true values scatter plot:

[[file:figs/baseline_validation_scatter.png]]

Test set predicted values vs. true values scatter plot:

[[file:figs/baseline_test_scatter.png]]

Validation set total daily sales:

[[file:figs/baseline_validation.png]]

Test set total daily sales:

[[file:figs/baseline_test.png]]

** Add item categories and various time features

Observations:

- Excellent improvement in rmse on validation over baseline model.
- Predicted vs. True scatter also looks much better than the baseline model. The model is still over-predicting sales=1, but a lot more of the distribution is clustered along y=x (accurate predictions).
- From the scatter plot for the test set, we see that a single outlier is probably making test RSME look worse than it is. Test RSME still isn't too bad.
- Decent looking total daily sales graphs for validation and test.

Summary Statistics

number of trees: 98

train-rmse: 1.56050	
validation-rmse: 1.56527

train-mape: 0.21753	
validation-mape: 0.22912

train-mae: 0.36448	
validation-mae: 0.39369

test rsme: 2.315024722824876

[[file:figs/add_features_feature_importance.png]]

Validation set predicted values vs. true values scatter plot:

[[file:figs/add_features_validation_scatter.png]]

Test set predicted values vs. true values scatter plot:

[[file:figs/add_features_test_scatter.png]]

Validation set total daily sales:

[[file:figs/add_features_validation.png]]

Test set total daily sales:

[[file:figs/add_features_test.png]]

** Use only the most recent year's data

Observations:

- If anything, RMSE on test worsens slightly using only data from 2015. Perhaps this is just from giving more weight to the single outlier.
- MAPE and MAE improved noticeably from the model with feat engineering that uses all 3 years of data.

Summary statistics:

number of trees: 39

train-rmse: 1.47490		
validation-rmse: 1.47621

train-mape: 0.17846	
validation-mape: 0.17519

train-mae: 0.29268	
validation-mae: 0.34397

test rmse: 2.500863996930012

[[file:figs/add_features_2015_feature_importance.png]]

Validation set predicted values vs. true values scatter plot:

[[file:figs/add_features_2015_validation_scatter.png]]

Test set predicted values vs. true values scatter plot:

[[file:figs/add_features_2015_test_scatter.png]]

Validation set total daily sales:

[[file:figs/add_features_2015_validation.png]]

Test set total daily sales:

[[file:figs/add_features_2015_test.png]]

* Possible improvements

Ran out of time, but here are some ways to improve:

** Improve these models
- Figure out if we can remove the outlier in the test set. There are several entries with values like this in other months, so not immediately obvious if we can.
- Hyperparameter searches
- Cross validation across more months
- Look at days/items/shops where model performs worst for clues
- Add lag features
- Could see if making the data stationary helps
 
** Multi-step prediction
- We've done direct one-month ahead and two-month ahead prediction. Model can be adapted to direct N-month ahead predictions.
- Could try recursive N-month or N-day ahead predictions with this model.

** Other Tools
- ARIMA (maybe with just a few month's data)
- Prophet
- If time + compute, TFT looks cool and could be SOTA (as of 2020): https://arxiv.org/pdf/1912.09363v2.pdf
 
* Files in this Repository

- competitive-data-science-predict-future-sales.zip (raw data)
- train_clean.csv (cleaned data)
- eda.ipynb
- clean_data.py

Run clean_data.py to create train_clean.csv, used in xgboost.ipynb  
  
- xgboost.ipynb (models)
